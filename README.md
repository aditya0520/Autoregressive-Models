# Probabilistic Modeling and Text Generation with GPT-2

## Overview
This project explores advanced concepts in probabilistic modeling and machine learning, focusing on Maximum Likelihood Estimation, KL Divergence, Logistic Regression, Bayesian Networks, Autoregressive Models, and Monte Carlo Integration. It also includes the development and evaluation of an autoregressive generative model leveraging GPT-2 for text generation and analysis.

## Contents
- **Written Solutions:** Theoretical explanations covering topics like MLE, KL Divergence, conditional independence, and autoregressive models.
- **Coding Implementations:** Python scripts for sampling using GPT-2, calculating log-likelihoods, and performing text classification tasks.
- **Extra Credit Tasks:** Advanced tasks on joint temperature scaling and multi-token temperature scaling.

## Key Concepts Explored
- **Maximum Likelihood Estimation & KL Divergence:** Demonstrating their equivalence under specific conditions.
- **Logistic Regression vs Naive Bayes:** Establishing the connection between generative and discriminative models.
- **Bayesian Networks:** Analyzing parameter requirements under conditional independence assumptions.
- **Autoregressive Models:** Comparing forward and reverse factorizations using neural networks.
- **Monte Carlo Integration:** Estimating marginal likelihoods with unbiased estimators.

## GPT-2 Based Implementations
- **Text Generation:** Created technical abstracts mimicking NeurIPS papers.
- **Log-Likelihood Calculation:** Evaluated the probabilistic quality of generated texts.
- **Text Classification:** Distinguished between random, Shakespearean, and academic texts.
- **Temperature Scaling:** Enhanced control over the diversity and coherence of generated sequences.

